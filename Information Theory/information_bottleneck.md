## Topic: Fairness

A compiled list of information bottleneck-related papers which I have read.

Last updated: 3 July 2024

### Icon description:

ü•á at least 1k citations (at the time of reading)

ü•à at least 500 citations (at the time of reading)

ü•â at least 50 citations (at the time of reading)

‚≠ê less than 50 citations (at the time of reading)

----

### Table of Contents:
- [Information Bottleneck Theory](#information-bottleneck-theory)

----

### Information Bottleneck Theory

* ü•á 2000: [The Information Bottleneck Method](https://arxiv.org/pdf/physics/0004057) <br>
This paper proposed the Information Bottleneck method to study the concept of relevant information quantitatively. It studies the trade-off between compression and prediction to find the concise representations for an input random variable that contain as much relevant information as possible for an output variable.

* ü•â 2001: [Multivariate Information Bottleneck](https://arxiv.org/pdf/1301.2270) <br>
This paper extended the orignal Information Bottleneck method to multivariate cases.


* ü•â 2002: [The Information Bottleneck: Theory and Applications](https://www.cs.huji.ac.il/labs/learning/Theses/Slonim_PhD.pdf) <br>
This is Noam Slonim's thesis which provides a comprehensive review of the Information Bottleneck method along with its multivariate extension.

* ü•â 2010: [Learning and Generalization with the Information Bottleneck](https://www.cs.huji.ac.il/labs/learning/Papers/ibgen.pdf) <br>
This paper studied the learning theoretic properties of the Information Bottleneck (IB) method. They proved several finite bounds that showed that the IB method can generalize quite using the plug-in estimates, even with sample sizes much smaller than the ones required for reliable estimation of joint distribution. They also argued that in the supervised learning setting, the IB method can be used to study the performance-complexity trade-off.

* 2015: [Deep Learning and the Information Bottleneck Principle](https://arxiv.org/pdf/1503.02406) <br>
This paper proposed to analyze the deep neural networks (DNNs) using the Information Bottleneck (IB) method. This was by visualizing the information curve, which plots the mutual information between the layers and the input and output variables. They applied the IB method to quantify the efficiency/optimality of the internal representations of the hidden layers in the DNNs.

